{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ac08b-5193-47a1-9e7b-3187c1c58aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-small-hi/checkpoint-1000\")\n",
    "\n",
    "# Apply pruning to the model's first convolutional layer\n",
    "prune.l1_unstructured(model.conv1, name=\"weight\", amount=0.3)\n",
    "prune.remove(model.conv1, 'weight')  # Make pruning permanent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e936149-06bd-47d4-a44d-6f931d029aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"./src/checkpoint-9000\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "\n",
    "model.eval()  # Ensure the model is in evaluation mode for quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear, nn.Conv2d, nn.EmbeddingBag, nn.LSTM, nn.GRU}, dtype=torch.qint8)\n",
    "\n",
    "# save config\n",
    "quantized_model.config.save_pretrained(\"whisper-quantized-config\")\n",
    "# save state dict\n",
    "quantized_state_dict = quantized_model.state_dict()\n",
    "torch.save(quantized_state_dict, \"whisper-quantized.pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# quantized_model.save_model('./whisper-small-hi-quantized')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d56118-bc76-4dcc-a918-b7802fbec6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "# load config and dummy model\n",
    "config = AutoConfig.from_pretrained(\"whisper-quantized-config\")\n",
    "dummy_model = WhisperForConditionalGeneration(config)\n",
    "\n",
    "reconstructed_quantized_model = torch.quantization.quantize_dynamic(\n",
    "    dummy_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "reconstructed_quantized_model.load_state_dict(quantized_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b3673-8c84-45a2-a9a9-e17124f8df88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchaudio import transforms\n",
    "from datasets import DatasetDict\n",
    "import jiwer\n",
    "from jiwer import wer\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "def predict_audio_from_file(file_path, model):\n",
    " \n",
    "    speech_array, sampling_rate = torchaudio.load(file_path)\n",
    "    \n",
    "    # resample to 16000 hz (required by model)\n",
    "    if sampling_rate != 16000:\n",
    "        transform = transforms.Resample(sampling_rate, 16000)\n",
    "        speech_array = transform(speech_array)\n",
    "        \n",
    "        \n",
    "    sample_audio = DatasetDict({\n",
    "        'array': speech_array.squeeze(0),\n",
    "        'sampling_rate': 16000\n",
    "    })\n",
    "    \n",
    "    input_features = processor(sample_audio[\"array\"], sampling_rate=sample_audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "    # input_features = input_features.to(device)\n",
    "    \n",
    "    # generate predicted token ids\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    # decode predicted token ids to text\n",
    "    prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a7f0d-cfef-41c5-a027-ad2c6b5bace4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"./src/checkpoint-9000\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Normal Time Taken (Load Model): {end - start:.2f}s\")\n",
    "start = time.time()\n",
    "\n",
    "prediction = predict_audio_from_file('audio_2.m4a', model)\n",
    "transcript = \"Heading is one seven zero, target is purple, blue, grey fighter jet, tool to deploy is electromagnetic pulse.\"\n",
    "print(f\"Actual: {transcript}\\n\")\n",
    "print(f\"Prediction: {prediction}\\n\")\n",
    "print(f\"WER%: {100* wer(transcript, prediction)}\\n\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Normal Time Taken (Inference): {end - start:.2f}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4a409-e5fd-4eb4-8e9b-b89c8f5a34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"./src/checkpoint-9000\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "\n",
    "model.eval()  # Ensure the model is in evaluation mode for quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear, nn.Conv2d, nn.EmbeddingBag, nn.LSTM, nn.GRU}, dtype=torch.qint8)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Quantized Time Taken (Load Model): {end - start:.2f}s\")\n",
    "start = time.time()\n",
    "\n",
    "prediction = predict_audio_from_file('audio_2.m4a', quantized_model)\n",
    "transcript = \"Heading is one seven zero, target is purple, blue, grey fighter jet, tool to deploy is electromagnetic pulse.\"\n",
    "print(f\"Actual: {transcript}\\n\")\n",
    "print(f\"Prediction: {prediction}\\n\")\n",
    "print(f\"WER%: {100* wer(transcript, prediction)}\\n\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Quantized Time Taken (Inference): {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d583e09c-7dbc-4c3b-8cb6-342cd3f92fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/novice /home/jupyter/til-24-base/asr\n",
      "{'key': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'audio': ['audio_0.wav', 'audio_1.wav', 'audio_2.wav', 'audio_3.wav', 'audio_4.wav', 'audio_5.wav', 'audio_6.wav', 'audio_7.wav', 'audio_8.wav', 'audio_9.wav'], 'transcript': ['Heading is one five zero, target is green commercial aircraft, tool to deploy is electromagnetic pulse.', 'Heading is two six zero, target is black, white, and yellow commercial aircraft, tool to deploy is surface-to-air missiles.', 'Heading is one zero five, target is silver, green, and yellow light aircraft, tool to deploy is anti-air artillery.', 'Heading is two niner zero, target is brown and blue cargo aircraft, tool to deploy is electromagnetic pulse.', 'Heading is zero one five, target is yellow camouflage drone, tool to deploy is EMP.', 'Heading is two seven five, target is purple, orange, and blue cargo aircraft, tool to deploy is interceptor jets.', 'Heading is one seven five, target is black, blue, and grey fighter jet, tool to deploy is machine gun.', 'Heading is three two zero, target is purple and brown cargo aircraft, tool to deploy is surface-to-air missiles.', 'Heading is one zero zero, target is blue, brown, and grey commercial aircraft, tool to deploy is electromagnetic pulse.', 'Heading is one three zero, target is orange and grey missile, tool to deploy is machine gun.']}\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import torchaudio\n",
    "from datasets import Dataset, load_metric, DatasetDict\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import jiwer\n",
    "import base64\n",
    "\n",
    "\n",
    "# Define the path to the directory\n",
    "current_directory = Path.cwd()\n",
    "file_path = current_directory / '..' / '..' / 'novice'\n",
    "data_dir = file_path.resolve()\n",
    "print(data_dir, current_directory)\n",
    "\n",
    "# Read data from a jsonl file and reformat it\n",
    "data = {'key': [], 'audio': [], 'transcript': []}\n",
    "with jsonlines.open(data_dir / \"asr.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        if len(data['key']) < 10: \n",
    "            for key, value in obj.items():\n",
    "                data[key].append(value)\n",
    "                \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b3e85e-aff7-43eb-93dd-4e018aec5cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Quantized Time Taken (Load Model): 9.09s\n",
      "Heading is one five zero, target is green commercial aircraft, tool to deploy is electromagnetic pulse.\n",
      "Heading is two six zero, target is black, white, and yellow commercial aircraft, tool to deploy is surface-to-air missiles.\n",
      "Heading is one zero five, target is silver, green, and yellow light aircraft, tool to deploy is anti-air artillery.\n",
      "Heading is two niner zero, target is brown and blue cargo aircraft, tool to deploy is electromagnetic pulse.\n",
      "Heading is zero one five, target is yellow camouflage drone, tool to deploy is EMP.\n",
      "Heading is two seven five, target is purple, orange, and blue cargo aircraft, tool to deploy is interceptor jets.\n",
      "Heading is one seven five, target is black, blue, and grey fighter jet, tool to deploy is machine gun.\n",
      "Heading is three two zero, target is purple and brown cargo aircraft, tool to deploy is surface-to-air missiles.\n",
      "Heading is one zero zero, target is blue, brown, and grey commercial aircraft, tool to deploy is electromagnetic pulse.\n",
      "Heading is one three zero, target is orange and grey missile, tool to deploy is machine gun.\n",
      "Actual: Heading is one seven zero, target is purple, blue, grey fighter jet, tool to deploy is electromagnetic pulse.\n",
      "\n",
      "Prediction: Heading is one three zero, target is orange and grey missile, tool to deploy is machine gun.\n",
      "\n",
      "Reconstructed Quantized Time Taken (Inference): 42.97s\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "from torchaudio import transforms\n",
    "from datasets import DatasetDict\n",
    "# import jiwer\n",
    "# from jiwer import wer\n",
    "# from functools import reduce\n",
    "# from pathlib import Path\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def predict_audio_from_file(file_path, model):\n",
    " \n",
    "    speech_array, sampling_rate = torchaudio.load(file_path)\n",
    "    \n",
    "    # resample to 16000 hz (required by model)\n",
    "    if sampling_rate != 16000:\n",
    "        transform = transforms.Resample(sampling_rate, 16000)\n",
    "        speech_array = transform(speech_array)\n",
    "        \n",
    "        \n",
    "    sample_audio = DatasetDict({\n",
    "        'array': speech_array.squeeze(0),\n",
    "        'sampling_rate': 16000\n",
    "    })\n",
    "    \n",
    "    input_features = processor(sample_audio[\"array\"], sampling_rate=sample_audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "    # input_features = input_features.to(device)\n",
    "    \n",
    "    # generate predicted token ids\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    # decode predicted token ids to text\n",
    "    prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# load config and dummy model\n",
    "config = AutoConfig.from_pretrained(\"whisper-quantized-config\")\n",
    "dummy_model = WhisperForConditionalGeneration(config)\n",
    "\n",
    "reconstructed_quantized_model = torch.quantization.quantize_dynamic(\n",
    "    dummy_model,  {nn.Linear, nn.Conv2d, nn.EmbeddingBag, nn.LSTM, nn.GRU}, dtype=torch.qint8\n",
    ")\n",
    "reconstructed_quantized_model.load_state_dict(torch.load(\"whisper-quantized.pt\"))\n",
    "reconstructed_quantized_model.eval()\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Reconstructed Quantized Time Taken (Load Model): {end - start:.2f}s\")\n",
    "start = time.time()\n",
    "for i in data['audio']:\n",
    "    prediction = predict_audio_from_file(data_dir / 'audio' / i, reconstructed_quantized_model)\n",
    "    print(prediction)\n",
    "transcript = \"Heading is one seven zero, target is purple, blue, grey fighter jet, tool to deploy is electromagnetic pulse.\"\n",
    "print(f\"Actual: {transcript}\\n\")\n",
    "print(f\"Prediction: {prediction}\\n\")\n",
    "# print(f\"WER%: {100* wer(transcript, prediction)}\\n\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Reconstructed Quantized Time Taken (Inference): {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38893ecc-bed9-4d9c-8596-6e791adfe47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import torchaudio\n",
    "from datasets import Dataset, load_metric, DatasetDict\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import jiwer\n",
    "import base64\n",
    "\n",
    "\n",
    "# Define the path to the directory\n",
    "current_directory = Path.cwd()\n",
    "file_path = current_directory / '..' / '..' / 'novice'\n",
    "data_dir = file_path.resolve()\n",
    "print(data_dir, current_directory)\n",
    "\n",
    "# Read data from a jsonl file and reformat it\n",
    "data = {'key': [], 'audio': [], 'transcript': []}\n",
    "with jsonlines.open(data_dir / \"asr.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        if len(data['key']) < 10: \n",
    "            for key, value in obj.items():\n",
    "                data[key].append(value)\n",
    "\n",
    "data2 = {\"instances\": []}                \n",
    "for j, i in enumerate(data['key']):\n",
    "    with open(data_dir / 'audio' / data['audio'][j], \"rb\") as file:\n",
    "        audio_bytes = file.read()\n",
    "        instance = {\n",
    "            \"key\": i,\n",
    "            \"b64\": base64.b64encode(audio_bytes).decode(\"ascii\"),\n",
    "            \"transcript\": data['transcript'][j]\n",
    "        }\n",
    "        data2['instances'].append(instance)\n",
    "        \n",
    "\n",
    "# Convert to a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data2) # converts it into a dataset object which has in-built helper functions to help us later on when we need to do operations on it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae5663-7201-4844-b069-80327217ef6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[1]['instances']['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc482b4-0202-446b-aea7-af74d2fb41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def run_inference(rank, world_size):\n",
    "    # create default process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "    \n",
    "    # load a model \n",
    "    model = reconstructed_quantized_model\n",
    "    # model.load_state_dict(PATH)\n",
    "    model.eval()\n",
    "    model.to(rank)\n",
    "\n",
    "    # create a dataloader\n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=4)\n",
    "    print(loader)\n",
    "\n",
    "    # iterate over the loaded partition and run the model\n",
    "    for idx, data in enumerate(loader):\n",
    "        print(data)\n",
    "        pass\n",
    "    \n",
    "def main():\n",
    "    world_size = 4\n",
    "    mp.spawn(run_inference,\n",
    "        args=(world_size,),\n",
    "        nprocs=world_size,\n",
    "        join=True)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7a8cb-af38-435d-936f-aff97d71745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc118a5a-1409-47b4-822b-cd000974db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# load fine-tuned model and tokenizer\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_ckpt)\n",
    "# quantize model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
